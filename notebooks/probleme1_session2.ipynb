{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Problème - Session n°2 : une variable cachée"
      ],
      "metadata": {
        "id": "GYE9s4-HA1o_"
      },
      "id": "GYE9s4-HA1o_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce problème, on travaille sur un jeu de données comportant 50.000 entrées $x_i$ et des cibles $y_i$. Les entrées sont des vecteurs de taille 10 (au format torch), les cibles sont des scalaires construits à partir de cinq fonctions différentes ($f_0$, ..., $f_4$) : \\\n",
        "\n",
        "$$ \\forall i, \\exists k\\in [\\![0 \\;;4]\\!]  \\:\\: \\text{tel que} \\: f_k(x_i) = y_i $$\n",
        "\n",
        "Ces fonctions sont inconnues, ainsi que l'indice $k$. Par contre, on sait que le groupe des 1000 premières cibles ont été construites à partir du même indice  $k$, de même pour les mille  suivantes, et ainsi de suite.\n",
        "\n",
        "Le but est de parvenir à rassembler les groupes de cibles qui ont été générées avec le même indice $k$ (avec la même fonction)."
      ],
      "metadata": {
        "id": "pknhArHLLPP-"
      },
      "id": "pknhArHLLPP-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example d'échantillonnage du dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "! git clone https://github.com/nouhalahyen/exam_2025_session2.git\n",
        "! cp exam_2025_session2/utils/utils.py .\n",
        "from utils import Problem1Dataset\n",
        "\n",
        "dataset = Problem1Dataset()\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "for batch in dataloader:\n",
        "    x_batch, y_batch, k_batch, idx_batch = batch\n",
        "    print(\"Batch input shape:\", x_batch.shape)\n",
        "    print(\"Batch target shape:\", y_batch.shape)\n",
        "    print(\"Batch k shape:\", k_batch.shape) # indice k (pas utilisable à l'entraînement)\n",
        "    print(\"Batch indices shape:\", idx_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "zqFIZKgTPK14",
        "outputId": "f85014f3-6fb6-45b2-e9cc-dfe9acb804b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zqFIZKgTPK14",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exam_2025_session2'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 89 (delta 5), reused 4 (delta 4), pack-reused 80 (from 2)\u001b[K\n",
            "Receiving objects: 100% (89/89), 570.04 KiB | 4.35 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n",
            "Batch input shape: torch.Size([32, 10])\n",
            "Batch target shape: torch.Size([32, 1])\n",
            "Batch k shape: torch.Size([32])\n",
            "Batch indices shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Consignes :**\n",
        "- Entraîner l'architecture proposée dans la cellule suivante.\n",
        "- Montrer que les vecteurs 2D de self.theta permettent de répondre\n",
        "  au problème posé.\n",
        "- Décrire le rôle de self.theta, du vector noise \\\n",
        " et ainsi que la raison de la division par 1000 (**indices // 1000** dans le code)."
      ],
      "metadata": {
        "id": "-q1zuebPPvob"
      },
      "id": "-q1zuebPPvob"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
        "        super(DeepMLP, self).__init__()\n",
        "        self.theta = nn.Parameter(torch.randn(50, 2))\n",
        "        self.fc1 = nn.Linear(input_dim + 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, indices):\n",
        "        theta_batch = self.theta[indices // 1000, :]\n",
        "        noise = torch.normal(mean=torch.zeros_like(theta_batch),\n",
        "                             std=torch.ones_like(theta_batch))\n",
        "        x = torch.cat([x, theta_batch + noise], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x, theta_batch"
      ],
      "metadata": {
        "id": "idbO80HfPdiQ"
      },
      "id": "idbO80HfPdiQ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "self.theta → self.theta est une matrice de taille (50, 2) qui représente des vecteurs latents associés aux groupes de 1000 échantillons.\n",
        "Chaque ligne de self.theta correspond à un groupe de 1000 échantillons et sert de signature unique pour ce groupe. self.theta Ajoute une information latente pour représenter chaque groupe.\n",
        ". \\\\\n",
        "\n",
        "noise → Empêche la mémorisation et améliore la généralisation. (Prévient l'overfitting) \\\\\n",
        "\n",
        "indices // 1000 → Associe le même vecteur theta aux groupes de 1000 échantillons pour capturer leur structure cachée. en effet, L’indice idx_batch correspond aux indices globaux des données (0 à 49 999).\n",
        "En faisant indices // 1000, on obtient un nombre entre 0 et 49, ce qui permet :\n",
        "D’assigner un même theta aux 1000 échantillons d’un même groupe.\n",
        "De garantir que chaque bloc de 1000 exemples utilise le même vecteur latent de theta, ce qui permet au modèle de détecter la structure des groupes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EIVX2aYqR0T7"
      },
      "id": "EIVX2aYqR0T7"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialiser le modèle, la fonction de perte et l'optimiseur\n",
        "model = DeepMLP(input_dim=10, output_dim=1)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Définir les paramètres d'entraînement\n",
        "batch_size = 64  # Vous pouvez ajuster la taille du batch\n",
        "epochs = 100  # Vous pouvez augmenter le nombre d'époques\n",
        "\n",
        "# Créer le DataLoader\n",
        "dataset = Problem1Dataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Entraîner le modèle\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        x_batch, y_batch, _, idx_batch = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, theta_batch = model(x_batch, idx_batch)\n",
        "        loss = criterion(output.squeeze(), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# Visualiser les vecteurs theta après l'entraînement\n",
        "theta_values = model.theta.detach().numpy()\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(theta_values[:, 0], theta_values[:, 1], c=range(50), cmap='viridis', s=50)\n",
        "plt.colorbar(label=\"Group Index\")\n",
        "plt.xlabel(\"Theta Dim 1\")\n",
        "plt.ylabel(\"Theta Dim 2\")\n",
        "plt.title(\"Visualisation des Vecteurs Theta\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0r4lF81gUw5c",
        "outputId": "92556ea4-0202-420e-99cc-0a4ce96fa4f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0r4lF81gUw5c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 180.9774\n",
            "Epoch 2/100, Loss: 174.9205\n",
            "Epoch 3/100, Loss: 174.2381\n",
            "Epoch 4/100, Loss: 174.1938\n",
            "Epoch 5/100, Loss: 174.3409\n",
            "Epoch 6/100, Loss: 173.9078\n",
            "Epoch 7/100, Loss: 173.6738\n",
            "Epoch 8/100, Loss: 173.6752\n",
            "Epoch 9/100, Loss: 173.6289\n",
            "Epoch 10/100, Loss: 173.5973\n",
            "Epoch 11/100, Loss: 173.5147\n",
            "Epoch 12/100, Loss: 173.5775\n",
            "Epoch 13/100, Loss: 173.4692\n",
            "Epoch 14/100, Loss: 173.7033\n",
            "Epoch 15/100, Loss: 173.5190\n",
            "Epoch 16/100, Loss: 173.5235\n",
            "Epoch 17/100, Loss: 173.3754\n",
            "Epoch 18/100, Loss: 173.5140\n",
            "Epoch 19/100, Loss: 173.3985\n",
            "Epoch 20/100, Loss: 173.4482\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}